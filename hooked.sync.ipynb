{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa84529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from transformer_lens import HookedTransformer\n",
    "from record_utils import record_activations\n",
    "from explore_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea1ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = \"checkpoints/TinyZero/v4/actor/global_step_300\"\n",
    "hooked_model_path = \"checkpoints/TinyZero/v4/actor/hooked_global_step_300.pt\"\n",
    "data_path = \"data/train.parquet\"\n",
    "batch_size = 4\n",
    "valid_size = 100\n",
    "max_prompt_length = 256\n",
    "max_response_length = 150\n",
    "max_new_tokens = max_response_length\n",
    "probe_path = \"probe_checkpoints/v2/probe.pt\"\n",
    "n_layers = 36\n",
    "record_module_names = [f\"model.layers.{idx}\" for idx in range(n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f28ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home01/ajyl/TinyZero/explore_utils.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.device(\"cuda:0\"):\n",
    "    hooked_actor = load_hooked_model(hooked_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f6f7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58869b217577423fba1733e460de36ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with torch.device(\"cuda:1\"):\n",
    "    actor = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67c08a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset len: 327680\n",
      "filter dataset len: 327680\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, valid_dataloader = get_dataloader(\n",
    "    data_path, batch_size, max_prompt_length, valid_size, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3f1f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_809191/4123695670.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  probe = torch.load(probe_path).detach().cuda()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probe = torch.load(probe_path).detach().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12aaf903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = valid_dataloader.dataset[0]\n",
    "\n",
    "input_ids = batch[\"input_ids\"].unsqueeze(0).cuda()\n",
    "attention_mask = batch[\"attention_mask\"].unsqueeze(0).cuda()\n",
    "\n",
    "generation_config = GenerationConfig(do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0997a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_size = actor.config.max_position_embeddings\n",
    "max_new_tokens = 30\n",
    "orig_output = generate(\n",
    "    actor,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    max_new_tokens,\n",
    "    block_size,\n",
    "    tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27d3870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:34<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def tl_generate(\n",
    "    tl_model,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    max_new_tokens,\n",
    "    block_size,\n",
    "    eos_token_id,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using a transformer language model with greedy sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The auto-regressive transformer model that outputs logits.\n",
    "        input_ids: A tensor of shape (batch_size, sequence_length) representing the initial token indices.\n",
    "        max_new_tokens: The number of new tokens to generate.\n",
    "        block_size: The maximum sequence length (context window) the model can handle.\n",
    "        device: The device on which computations are performed.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the original context concatenated with the generated tokens.\n",
    "    \"\"\"\n",
    "    device = \"cuda\"\n",
    "    tl_model.eval()  # Set the model to evaluation mode\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    batch_size = input_ids.shape[0]\n",
    "\n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in tqdm(range(max_new_tokens)):\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "        if input_ids.shape[1] > block_size:\n",
    "            idx_cond = input_ids[:, -block_size:]\n",
    "            attn_mask_cond = attention_mask[:, -block_size:]\n",
    "        else:\n",
    "            idx_cond = input_ids\n",
    "            attn_mask_cond = attention_mask\n",
    "\n",
    "        position_ids = attn_mask_cond.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attn_mask_cond == 0, 1)\n",
    "\n",
    "        # Get logits from the model. Ensure your model's forward function accepts an attention mask.\n",
    "        logits = tl_model(\n",
    "            idx_cond,\n",
    "            return_type=\"logits\",\n",
    "            attention_mask=attn_mask_cond,\n",
    "            prepend_bos=True,\n",
    "            padding_side=\"left\",\n",
    "            # position_ids=position_ids,\n",
    "        )\n",
    "        #logits = output[\"logits\"]\n",
    "        # Focus only on the last time step's logits\n",
    "        logits = logits[:, -1, :]  # shape: (batch, vocab_size)\n",
    "\n",
    "        # Greedy sampling: select the token with the highest logit\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)  # shape: (batch, 1)\n",
    "\n",
    "        new_finished = (~finished) & (next_token.squeeze(1) == eos_token_id)\n",
    "        finished |= new_finished\n",
    "        next_token[finished] = eos_token_id\n",
    "\n",
    "        # Append the predicted token to the sequence\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        new_mask = torch.ones(\n",
    "            (batch_size, 1), dtype=attention_mask.dtype, device=device\n",
    "        )\n",
    "        attention_mask = torch.cat([attention_mask, new_mask], dim=1)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "max_new_tokens = 200\n",
    "\n",
    "hooked_output = tl_generate(\n",
    "    hooked_actor,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    max_new_tokens,\n",
    "    block_size,\n",
    "    tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce261683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\\nUser: Using the numbers [2, 59, 55, 72], create an equation that equals 44. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\\nAssistant: Let me solve this step by step.\\n<think> We have the numbers 2, 59, 55, and 72. We need to use all of them to make an']\n",
      "[\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\\nUser: Using the numbers [2, 59, 55, 72], create an equation that equals 44. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\\nAssistant: Let me solve this step by step.\\n<think> We have the numbers 2, 59, 55, and 72. We need to use these numbers to make an equation that equals 44 using basic arithmetic operations. Let's try different combinations:\\n- 72 - 59 - 55 + 2 = 64 - 55 - 55 + 2 = 8 - 55 + 2 = -43 + 2 = -41 (not 44)\\n- 72 - 59 - 55 + 2 = 72 - 59 - 55 + 2 = 13 - 55 + 2 = -42 + 2 = -40 (not 44)\\n- 72 - 59 - 55 + 2 = 72 - 59 - 55 + 2 = 13 - 55 +\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.batch_decode(orig_output, skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(hooked_output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa0c7c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2MLP(\n",
       "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "  (act_fn): SiLU()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.model.layers[0].mlp.act_fn.hook_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfbec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
